好,那今天要跟大家分享 Frugal GPT,  來看看說真正的窮人是怎麼在用 Chair GPT 的那這個 Frugal 這個字呢,就是節儉的意思,所以從他的名字就知道說,這個 是真正的窮人的 Chair GPT這篇 paper 是來自一個 Stanford 的 paper,然後是前幾天被放到 archive 上面的,然後把論 文連結放在這邊那從他的 title 你就可以知道說他想做的事情是什麼呢?他想做的事情是這樣How to use large language model while reducing cost and improving performance也就是說,現在大家都 call 這個 Chair GPT 的 API,那你一樣 call Chair GPT 的 API但有沒有什麼比較 common 的方法,少花一些錢就做一樣的事情呢?這就是這篇 paper 想要告訴我們的好,那為什麼這篇 paper 是重要的呢?因為其實使用 large language model 的 API 也可能是可以很花錢的舉例來說,GPT-4 它是每輸入一千個 token 算你 0.03 美金,每輸出一千個 token 算你 0.06 美金它輸入跟輸出花的錢是不一樣的, 這個錢乍聽之下其實沒有很多舉例來說,假設每次使用的時候你都輸入一千個 token,讓他輸出一千個 token那你花的錢呢,平均就是 0.09 元,也就是 2.78 元的新台幣,那是真的不多但是一旦量很大的時候,雖然每一次使用這個 API 花的錢不多,一旦量很多的時候,也許也是很可觀的那我一時之間想不到非常好的例子吧我就想到前一陣子桃園市政府有說,以後要用 Check GPT 來分析 1999 的電話紀錄但是如果是拿來分析台北市的 1999 的陳情案件的話,那可能就非常可觀了因為 1999 台北市民當家熱線,根據以下的資料,平均每個月是服務 15 萬通電話那假設每一通電話,你要把他的主字稿拿去做一下分析, 那每次你就要花大約 3 塊錢那 15 萬通電話,平均每個月大概要花 40 萬那對台北市政府來說,我相信這是一個小數字啦但是假設這是一個新創的服務的話,假設有一個 staff 要做這件事情的話,那顯然就是一個非常重的負擔所以有人就在想說, 怎麼同樣是 core API,但是省著點 core API那在這篇 paper 裡面呢,他就提到了三種方法那只有第三種方法他有做實驗啦,前面兩種方法就是告訴你一個概念第一個方法呢,他稱之為 prompt adaptation說穿了就是,既然輸入是要花錢的,怎麼少 花一點錢呢?就是輸入短一點怎麼輸入短一點呢?舉例來說,當你在做 in-context learning 的時候我們之前有講說,你可以提供給 language model 一些 example 讓他做得更好一點,這就是 in-context learning舉例來說,你先提供兩個 example,Q1 A1 Q2 A2,然後給 language model Q3,讓他產生第三個問題的答案但是他說,你可以想一下說,你真的需要兩個 example 嗎?如果今天只要一個 example 就可以得到差不多的結果,要不要放一個 example 就好了他把這個技術呢,叫做 prompt selection但是至於要怎麼 select prompt,兩個 example 裡面要選哪一個比較好這邊 paper 並沒有提到,他只是提一個概念,告訴你說也許你不用那麼多的 prompt,少放一點 prompt,你就省一點錢我們一般在使用這個線上的 API 的時候假設你 有一個 Q3,那你就是把 Q3 跟他的這個 example 一起丟到 large language model 得到 Q3 的答案有一個新的問題叫做 Q4,那你就把 Q4 丟到 large language model 得到他的答案那在這篇 paper 裡面提到一個方法,叫做 query concatenation,把輸入接起來這個輸入接起來是怎麼運作的呢?就是你不要把 Q3 跟 Q4 分開丟進去你把 Q3 跟 Q4 接起來,一起丟進 language model,同時得到答案 A3 跟 A4我本來剛看到這個方法的時候,我想說這樣有比較省錢嗎?這樣不就只是朝三暮四而 已嗎?本來 Q3 Q4 分開丟,現在串起來一起丟,輸入變長了,不是還是花比較多的錢嗎?但是這邊真正省錢的是在你的 context本來這個 in context learning 裡面的這個 example 要丟兩次當你把 Q3 Q4 接在一起,叫這個 Chair GPT 同時回答兩 個問題的時候你的 example 就只要丟一次而已,就省下丟 example 的錢這個窮人的思維不得了,他在每個地方都想要省錢 這個真的是我平常沒有辦法想到的這個方法二呢?方法二其實反而是比較花錢的方法二就是說,你不想用線上的 API那你不 就自己建一個 language model,就不用 call 線上的 API 啦那怎麼自己建一個 language model?我們之前講過了,就是窮 人怎麼在低資源的情況下,自己複刻一個 Chair GPT當然這個方法也是要耗費比較大的成本啦這個只有比較有錢的窮人才能夠使用這個方法好,但在這篇 paper 裡面呢,他提供了一個更窮酸的方法就是你自己還要 train 一個 language model那顯然你自己也是要花一些 computing resource 的錢的所以他教你一個更窮酸的方法這個方法就是,你把問過 Chair GPT 的 問題呢跟他的答案呢,通通都記錄下來,存在一個 database 裡面之後有人再問一個新的問題,叫做 Q-plan就看 database  裡面有沒有很類似的問題如果有很類似的問題,就不要再 call Chair GPT 了把之前看過的問題的答案丟出來,就當作是答 案了省了 call Chair GPT 的錢這個非常的 low tech,但是可以省錢啦假設說你這個 service 常常有使用者問非常類似的問題的話這一招也許可以幫你省不少錢啦好,那這個第三招呢,是這篇 paper 真正想要提出的方法他叫做 LLM Cascade他說現在線上有這麼多的 API 可以用他這邊呢,就列了各式各樣的 API那這些 API 的能力呢,有強有弱有小的模型,也有大的模型那每一個模型的收費都不太一樣那這個表格其實有錯啦這邊不是 10個 million 的 token 花的錢啦這個是一個 million 的 token 花的錢啦那通常這些 API 他們的收費呢,都分成三個面向第一個面向是,看你的輸入有多長根據輸入的長度收費另外一個是根據輸出的長度收費那你會發現輸入跟輸出的長度呢,往往是分開計價的那甚至有的 API 是你輸入很長也沒關 係,不跟你收錢但是如果他回答很長的話,就要跟你收很多錢我不知道為什麼會這樣子啦難道他們覺得輸出的成本比輸入更 高嗎?所以可能輸出的成本是比較高的,所以輸出的時候要花比較多錢那我想過另外一個比較邪惡的可能性就是輸入的長度 你可以自己控制嘛輸出的長度你沒辦法自己控制啊所以他到時候就讓那個 Lightning Model 都特別故意噴那個很長的答案才可以跟你收一大堆錢這樣子那還有的就是以次計費啦就是每次你 call 他每次的時候都要跟你收一點錢所以每一個 API 他花的錢呢都是不一樣就算同樣是 OpenAI 的 API比較小的模型,比如說 GPT-Query 只有 6 個 Billion 的參數相較於 GPT-3 還有 GPT-4 他就便宜很多GPT-3 你要 call GPT-3 他要的錢是 Query 的 10 倍所以就算是同一家公司出的 API你需 要花的錢其實也有可能有很大的不同所以怎麼辦呢?這邊 Paper 就是想要綜合不同的模型一起使用那為什麼綜合不同的模 型一起使用有可能比較省錢呢?這邊的技巧是這個樣子的因為這個殺雞不用牛刀如果我們知道有一些 API 比較弱,但是他比較便宜有些 API 比較強,但他比較貴那我們就應該拿簡單的問題去問比較弱的模型只有難的問題才問比較強的模型所以殺 雞的時候就用殺雞的刀殺牛的時候才把殺牛的刀拿出來而且不同的模型之間,他的能力可能是可以互補的在那篇論文裡面他就提供了一個這樣子的表格那這個表格呢,是讓這一些模型坐在某一個任務上這個任務叫做 Headline這個任務是讓這些模 型去讀一個新聞的標題然後根據這個新聞的標題判斷說黃金的價格是上漲還是下跌還是持平那這個任務呢,橫軸是指某一個模型他沒有辦法答對但其他模型可以答對的百分比比如說這邊橫軸是GPT-4,縱軸是什麼?縱軸是GPT-3,那你發現這邊寫4這 個4的意思就是說,對在這個任務裡面有4%的問題是GPT-4沒有答對而GPT-3答對的所以如果你看這邊的數值越小就代表在橫 軸在這個Row上面的這個模型是越強的那從這個表上可以看出說GPT-3、4還有Chair GPT看起來是最強的但就算他們是最強 的,他們仍然不是完美的還是有一定比例的問題是GPT-4他會答錯而其他模型會答對的大家現在都覺得GPT-4非常厲害就像是個神一樣,都拿他當仲裁者來評估其他的模型但他畢竟不是一個神,他還是有時候會答錯的所以在有些狀況下,其他模型還是有可能超過GPT-4的所以這一篇Paper就是想辦法把所有的語言模型合在一起使用怎麼把所有的語言模型都合在一起使用呢?他的方法是這樣的當使用者輸入一個問題的時候把這個問題先輸入給一個最弱的模型他這邊認為比較弱的模型是GPT-J然後這個最弱的模型會輸出一個答案接下來他有一個scorer這個scorer他做的事情是去判斷這個答案是正確的可能性有多大所 以你有一個模組,這個模組會持輸入跟一個答案然後得到一個分數,這個分數代表這個答案是正確的可能性那你這邊你可能 會問說那這個驗證正確可能性的模組是不是也要call API也要花錢呢?有可能要花錢但在這篇Paper裡面他們驗證答案是否 正確的模組是用一個很小的模型是用一個Roberta一個非常小的這個Bird系列的模型來做驗證這件事情所以他們就沒有把這個驗證模組所花的computing resource把它考慮進去那也許是因為驗證這件事情比生成這件事情簡單所以驗證的模組可以 遠比生成的模組他的network架構更為簡單就可以達到一定程度的效果但是訓練這個驗證的模組你也是要花運算資源去訓練的反正這篇Paper就暫時不考慮這個問題我們就假設有一個這篇Paper比較像是提出一個概念告訴你說假設你有一個好的驗 證的模組那你可以決定說這個答案好不好那如果這個答案夠好了那就直接拿出來給人看當作最終的答案那因為這個驗證的 模組它輸出的是一個分數所以這邊還要決定一個threshold決定一個域值比如說在這個一例子裡面這個域值是0.96如果這個驗證模組輸出來的分數是大過0.96的那就代表說這個GPT data已經夠好了那就給人看那如果是小於0.96的就把同樣的問題 再去問J1L看看J1L有什麼樣的答案接下來再驗證一下J1L的答案有沒有可能是正確的如果覺得J1L的答案很有可能是正確的 就把J1L的答案輸出給使用者看那直到最後大家都沒有辦法了所有語言模型都搞不定的問題才去問GPT-4因為GPT-4是最強但是最貴的模型所以只有最難的問題才去問GPT-4比較簡單的模型就給GPT-J來解就好了那透過這個方法就是他這邊單純嘗試 把GPT-J、J1L跟GPT-4三個模組串在一起然後做在剛才提到的那個叫headline的data set上面他發現說如果你單純使用GPT-4你得到85.7%的正確率但是跑完整個data set你要花33塊錢如果你今天是用這個frugal GPT也就是把三個語言模型接在一 起首先你要花的錢是少很多的因為很多問題都給GPT-J解決掉了還沒有到GPT-4就已經被解掉了所以花的錢是比較少的另外 有趣的地方是其實frugal GPT把三個模型綜合在一起以後他得到的結果是比GPT-4還要更強的那是因為其實有很多問題是其實沒有很多啦有4%左右的問題是GPT-J可以答對而GPT-4沒有辦法答對的所以有很多狀況是那個問題如果你一路問到GPT-4你反而會得到錯的答案但是你剛問GPT-J就覺得答案是對的了GPT-J評分的模組就告訴你說這個答案已經夠好了就把答案輸出 去了反而會得到正確的答案所以綜合多語言模型並不是比較省錢而且結果還會比原來的使用GPT-4的狀況還要更好一點
